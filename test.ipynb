{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tanh, einsum\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange, repeat\n",
    "from einops_exts import rearrange_many\n",
    "\n",
    "\n",
    "# class CrossAttentionTransformerBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         lm_block: nn.Module,\n",
    "#         config: dict,\n",
    "#         token_dim: int = 4096,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.lm_block = lm_block\n",
    "#         self.cross_x_block = GatedCrossAttentionBlock(\n",
    "#             config, token_dim, **kwargs)\n",
    "#         self.media_locations = None\n",
    "#         self.visual_features = None\n",
    "\n",
    "#     def forward(self, embs, **kwargs):\n",
    "#         logits = self.cross_x_block(\n",
    "#             embs, self.media_locations, self.visual_features)\n",
    "#         out = self.lm_block(logits, use_cache=False, **kwargs)\n",
    "#         return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: TensorType[\"Batch\", \"Sequence\", \"Dim\"]):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MaskedCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_token_dim: int = 4096,\n",
    "        visual_token_dim: int = 2048,\n",
    "        num_heads: int = 8,\n",
    "        n_latents: int = 64, \n",
    "        head_dim: int = 64, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temp = 1 / (head_dim ** -0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.device = device\n",
    "\n",
    "        self.v_k_w = nn.Linear(\n",
    "            visual_token_dim, head_dim * num_heads * 2, bias=False)\n",
    "        self.q_w = nn.Linear(\n",
    "            text_token_dim, head_dim * num_heads, bias=False)\n",
    "        self.out = nn.Linear(head_dim *num_heads, text_token_dim, bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "                latent: TensorType[\"Batch\", \"Sequence\", \"TokenDim\"],\n",
    "                y: TensorType[\"Batch\", \"Sequence Length\", \"TokenDim\"],\n",
    "                media_mask: TensorType[\"Batch\", \"Sequence Length\"]\n",
    "                ):\n",
    "        visual_features = rearrange(latent, 'b t n d -> b (t n) d')\n",
    "        print(\"vs_shape\", visual_features.shape)\n",
    "        k, v = self.v_k_w(visual_features).chunk(2, dim=-1)\n",
    "        q = self.q_w(y)\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h=8)\n",
    "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
    "        media_time = torch.arange(3).to(self.device) + 1\n",
    "        text_to_media_mask = rearrange(media_mask, 'b i -> b 1 i 1') == repeat(media_time, 'j -> 1 1 1 (j m)', m=64)\n",
    "        sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n",
    "        logits = sim.softmax(dim=-1)\n",
    "        text_without_media_mask = media_mask == 0\n",
    "        text_without_media_mask = rearrange(text_without_media_mask, 'b i -> b 1 i 1')\n",
    "        logits = logits.masked_fill(text_without_media_mask, 0.)\n",
    "        logits = einsum('... i j, ... j d -> ... i d', logits, v)\n",
    "        logits = rearrange(logits, 'b h n d -> b n (h d)')\n",
    "        y = self.out(logits)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedCrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, config, text_token_dim):\n",
    "        super().__init__()\n",
    "        self.x_attn = MaskedCrossAttention(text_token_dim = text_token_dim, n_latents = config['n_latents'])\n",
    "        self.tanh1 = nn.Parameter(torch.tensor([0.]))\n",
    "        self.ffw = FeedForward(dim=text_token_dim)\n",
    "        self.tanh2 = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def perceiver_pipe(self, visual_features, media_mask):\n",
    "        self.media_mask = media_mask\n",
    "        self.visual_features = visual_features\n",
    "\n",
    "    def forward(self, embs: TensorType[\"Batch\", \"Sequence Length\", \"TokenDim\"]):\n",
    "        x_attn = self.x_attn(self.visual_features, embs, self.media_mask)\n",
    "        attn_out = embs + tanh(self.tanh1) * x_attn\n",
    "        x_ffw = attn_out + self.ffw(x_attn) * tanh(self.tanh2)\n",
    "\n",
    "        return x_ffw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERCEIVER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "from einops_exts import rearrange_many\n",
    "\n",
    "\n",
    "\n",
    "class PerceiverAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_dim,\n",
    "        output_dim,\n",
    "        num_heads=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temp = 1 / (output_dim ** -0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.v_k_w = torch.nn.Linear(\n",
    "            token_dim, output_dim * num_heads * 2, bias=False)\n",
    "        self.q_w = torch.nn.Linear(\n",
    "            token_dim, output_dim * num_heads, bias=False)\n",
    "        self.out = nn.Linear(output_dim*num_heads, token_dim, bias=False)\n",
    "\n",
    "    def forward(self, q: TensorType[\"Batch\", \"Number of Images\", \"OutputDim\", \"TokenDim\"], k_v: TensorType[\"Batch\", \"Number of Images\", \"Sequence\", \"TokenDim\"]):\n",
    "\n",
    "        # (batch, n_images, sequence_length + output_dim, embedding_dim)\n",
    "        k_v = torch.cat((k_v, q), dim=-2)\n",
    "        # (batch, n_images, sequence_length + output_dim, embedding_dim)\n",
    "        k, v = self.v_k_w(k_v).tensor_split(2, dim=-1)\n",
    "        q = self.q_w(q)  # (batch, n_images, output_dim, embedding_dim)\n",
    "\n",
    "        q, k, v = rearrange_many(\n",
    "            (q, k, v), 'b n s (h d) -> b n h s d', h=self.num_heads)\n",
    "\n",
    "        q = q * self.temp\n",
    "\n",
    "        # k = Image Input Sequence Dimension / q = Output Dimension\n",
    "        sm = torch.einsum(\"b n h q d,b n h k d->b n h q k\", q, k)\n",
    "        attn = sm.softmax(dim=-1)\n",
    "\n",
    "        ff_input_per_head = torch.einsum(\n",
    "            \"b n h q k,b n h k d -> b n h q d\", attn, v)\n",
    "        ff_input = rearrange(\n",
    "            ff_input_per_head, \"b n h s d -> b n s (h d)\", h=self.num_heads)\n",
    "        return self.out(ff_input)\n",
    "\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_dim,\n",
    "        num_layers:  int = 2,\n",
    "        latent_size: int = 64,\n",
    "        time: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learned_latents = nn.Parameter(\n",
    "            torch.randn(latent_size, token_dim))\n",
    "        self.time_embeddings = nn.Parameter(torch.rand(time, 1, token_dim))\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.perceiver_attention_layers = nn.ModuleList([])\n",
    "        for _ in range(num_layers):\n",
    "            self.perceiver_attention_layers.append(nn.ModuleList([\n",
    "                PerceiverAttentionBlock(\n",
    "                    token_dim=token_dim,  output_dim=latent_size),\n",
    "                PerceiverFeedForwardLayer(token_dim=token_dim)\n",
    "            ]))\n",
    "        self.normalize = nn.LayerNorm(token_dim)\n",
    "\n",
    "    def forward(self, x: TensorType[\"Batch\", \"Number of Images\", \"Time\", \"Sequence\", \"Token Dimesion\"]):\n",
    "        if x.ndim == 3:\n",
    "            x = x[:, None, None, :, :]\n",
    "\n",
    "        batch_size, number_of_images, time, sequence_length, token_dim = x.size()\n",
    "        print(x.shape)\n",
    "        x = rearrange(x, \"b n t s d -> b n (t s) d\")\n",
    "        latents = self.learned_latents.repeat(\n",
    "            batch_size, number_of_images, 1, 1)\n",
    "        x = x + self.time_embeddings[:number_of_images]\n",
    "\n",
    "        for att_module, ff_layer in self.perceiver_attention_layers:\n",
    "            latents = att_module(latents, x)\n",
    "            latents = ff_layer(latents)\n",
    "\n",
    "        return self.normalize(latents)\n",
    "\n",
    "\n",
    "class PerceiverFeedForwardLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_dim: int = 3027,\n",
    "        mult: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(token_dim)\n",
    "        self.inner = nn.Linear(token_dim, mult*token_dim, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.outer = nn.Linear(mult*token_dim, token_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.outer(self.act(self.inner(self.norm(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magma.config import MultimodalConfig\n",
    "from magma.utils import get_world_info\n",
    "from magma.utils import get_tokenizer\n",
    "from magma.datasets.dataset import ImgCptDataset\n",
    "from magma.image_encoders import clip_encoder\n",
    "from magma.transforms import get_transforms\n",
    "from magma.language_model import get_gptj\n",
    "import copy\n",
    "from torch import nn\n",
    "torch.set_default_dtype(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GPTJ language model...\n",
      "Done Fetching Model ... \n"
     ]
    }
   ],
   "source": [
    "encoder = clip_encoder(name=\"RN50\")\n",
    "\n",
    "lm = get_gptj(from_pretrained=True)\n",
    "\n",
    "transforms = get_transforms(\n",
    "            384, \n",
    "            'clip_resnet_large',\n",
    "            input_resolution=encoder.input_resolution,\n",
    "        )\n",
    "\n",
    "perceiver = PerceiverResampler(2048)\n",
    "tokenizer = get_tokenizer(\n",
    "            \"gpt2\", sequence_length=lm.config.max_position_embeddings)\n",
    "cross_attention_layers=[]\n",
    "transformer = lm.transformer.h\n",
    "config = MultimodalConfig.from_yml('/home/ml-mmeuer/adaptable_magma/fb20-dgx2-configs/Flamingo.yml')\n",
    "for l in range(len(lm.transformer.h)):\n",
    "    layer_norm = getattr(lm.transformer.h[l], \"ln_1\")\n",
    "    x_attn_block = GatedCrossAttentionBlock(\n",
    "        config=config.cross_attention_config,\n",
    "        text_token_dim=lm.config.hidden_size,\n",
    "    )\n",
    "    cross_attention_layers.append(l)\n",
    "    setattr(lm.transformer.h[l], 'ln_1', nn.Sequential(\n",
    "        *[x_attn_block, layer_norm]))\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\":[\"<|image|>\"]})\n",
    "img_token = tokenizer.encode(\"<|image|>\")[0]\n",
    "lm.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "dataset = ImgCptDataset('/home/ml-mmeuer/adaptable_magma/magma/datasets/coco_train_val', tokenizer=tokenizer, transforms=transforms, few_shot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Magma(nn.Module): \n",
    "    def __init__(self, lm, tokenizer, transforms, encoder, img_token, perceiver, cross_attention_layers):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.img_token = img_token\n",
    "        self.transforms = transforms\n",
    "        self.perceiver = perceiver\n",
    "        self.encoder = encoder\n",
    "        self.cross_attention_layers = cross_attention_layers\n",
    "\n",
    "        \n",
    "    def forward(self, images, captions): \n",
    "        input_embeddings = self.lm.transformer.wte(captions).half()\n",
    "        images = images[:,:,None,:,:,:]\n",
    "        media_pos = captions == img_token\n",
    "        media_mask = media_pos.cumsum(dim=-1)\n",
    "        flat_img = rearrange(images, \"b n t c h w -> ( b n t ) c h w \")\n",
    "        featurized_images = encoder(flat_img)\n",
    "        visual_embeddings = rearrange(featurized_images, \"(b n t) s d -> b n t s d\", b = images.shape[0], n = images.shape[1], t=images.shape[2])\n",
    "        # add cross attention\n",
    "        print(\"Input_embeddings\",visual_embeddings.shape)\n",
    "        visual_features = self.perceiver(\n",
    "            visual_embeddings\n",
    "        )\n",
    "        print(\"vf.dim\", visual_features.dtype)\n",
    "        for l in self.cross_attention_layers:\n",
    "            x_attn_block = getattr(\n",
    "                lm.transformer.h[l], 'ln_1')[0]\n",
    "\n",
    "            x_attn_block.perceiver_pipe(\n",
    "                visual_features, media_mask=media_mask)\n",
    "            \n",
    "        lm_outputs = self.lm(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "magma = Magma(tokenizer=tokenizer,transforms=transforms, lm=lm, encoder=encoder,img_token=img_token, perceiver=perceiver, cross_attention_layers=cross_attention_layers)\n",
    "magma = magma.to(\"cuda:3\")\n",
    "images, captions = dataset[1]\n",
    "images, captions = images.to('cuda:3').half(), captions.to('cuda:3', dtype=torch.int64)\n",
    "images = images[None, : , : , : , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_embeddings torch.Size([1, 3, 1, 49, 2048])\n",
      "torch.Size([1, 3, 1, 49, 2048])\n",
      "vf.dim torch.float32\n",
      "vs_shape torch.Size([1, 192, 2048])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Half but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m magma(images, captions)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [20], line 34\u001b[0m, in \u001b[0;36mMagma.forward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     28\u001b[0m     x_attn_block \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m     29\u001b[0m         lm\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh[l], \u001b[39m'\u001b[39m\u001b[39mln_1\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m     x_attn_block\u001b[39m.\u001b[39mperceiver_pipe(\n\u001b[1;32m     32\u001b[0m         visual_features, media_mask\u001b[39m=\u001b[39mmedia_mask)\n\u001b[0;32m---> 34\u001b[0m lm_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm(\n\u001b[1;32m     35\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minput_embeddings,\n\u001b[1;32m     36\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:920\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, embs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[39m    ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39m    ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 920\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    921\u001b[0m     input_ids,\n\u001b[1;32m    922\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    923\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    924\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    925\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    926\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    927\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    928\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    929\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    930\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    931\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    932\u001b[0m     embs\u001b[39m=\u001b[39;49membs,\n\u001b[1;32m    933\u001b[0m )\n\u001b[1;32m    934\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    936\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:755\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, embs)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39minputs, use_cache, output_attentions)\n\u001b[1;32m    753\u001b[0m         \u001b[39mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 755\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[1;32m    756\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    757\u001b[0m         hidden_states,\n\u001b[1;32m    758\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    759\u001b[0m         attn_mask,\n\u001b[1;32m    760\u001b[0m         head_mask[i],\n\u001b[1;32m    761\u001b[0m     )\n\u001b[1;32m    762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    764\u001b[0m         hidden_states,\n\u001b[1;32m    765\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    770\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:751\u001b[0m, in \u001b[0;36mGPTNeoModel.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_forward\u001b[39m(\u001b[39m*\u001b[39minputs):\n\u001b[1;32m    750\u001b[0m     \u001b[39m# None for past_key_value\u001b[39;00m\n\u001b[0;32m--> 751\u001b[0m     \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39;49minputs, use_cache, output_attentions)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:439\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    431\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    437\u001b[0m ):\n\u001b[1;32m    438\u001b[0m     residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 439\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(hidden_states)\n\u001b[1;32m    440\u001b[0m     attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\n\u001b[1;32m    441\u001b[0m         hidden_states,\n\u001b[1;32m    442\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    448\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [9], line 102\u001b[0m, in \u001b[0;36mGatedCrossAttentionBlock.forward\u001b[0;34m(self, embs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embs: TensorType[\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSequence Length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTokenDim\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m--> 102\u001b[0m     x_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_attn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_features, embs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmedia_mask)\n\u001b[1;32m    103\u001b[0m     attn_out \u001b[39m=\u001b[39m embs \u001b[39m+\u001b[39m tanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh1) \u001b[39m*\u001b[39m x_attn\n\u001b[1;32m    104\u001b[0m     x_ffw \u001b[39m=\u001b[39m attn_out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffw(x_attn) \u001b[39m*\u001b[39m tanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh2)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [9], line 73\u001b[0m, in \u001b[0;36mMaskedCrossAttention.forward\u001b[0;34m(self, latent, y, media_mask)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvs_shape\u001b[39m\u001b[39m\"\u001b[39m, visual_features\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     72\u001b[0m k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_k_w(visual_features)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_w(y)\n\u001b[1;32m     74\u001b[0m q, k, v \u001b[39m=\u001b[39m rearrange_many((q, k, v), \u001b[39m'\u001b[39m\u001b[39mb n (h d) -> b h n d\u001b[39m\u001b[39m'\u001b[39m, h\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m     75\u001b[0m sim \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39m\u001b[39m... i d, ... j d -> ... i j\u001b[39m\u001b[39m'\u001b[39m, q, k)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Half but found Float"
     ]
    }
   ],
   "source": [
    "magma(images, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dim = 2048\n",
    "t_dim = 4096\n",
    "n_latents = 64 \n",
    "num_heads = 8 \n",
    "v_k_w = nn.Linear(\n",
    "            v_dim, n_latents * num_heads * 2, bias=False)#.to('cuda:3',dtype=torch.float16)\n",
    "q_w = nn.Linear(\n",
    "    t_dim, n_latents * num_heads, bias=False)#.to('cuda:3',dtype=torch.float16)\n",
    "out = nn.Linear(n_latents*num_heads, t_dim, bias=False)#.to('cuda:3',dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lm.transformer.wte(captions).float()\n",
    "media_pos = captions == img_token\n",
    "media_mask = media_pos.cumsum(dim=-1)#.to('cuda:3')\n",
    "latent = torch.randn(3,3,64,2048)#.to('cuda:3', dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 4096])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_shape torch.Size([3, 192, 2048])\n"
     ]
    }
   ],
   "source": [
    "visual_features = rearrange(latent, 'b n s d -> b (n s) d')\n",
    "print(\"vs_shape\", visual_features.shape)\n",
    "k, v = v_k_w(visual_features).chunk(2, dim=-1)\n",
    "q = q_w(y)\n",
    "q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h=8)\n",
    "sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
    "media_time = torch.arange(3) + 1 #.to('cuda:3') + 1\n",
    "text_to_media_mask = rearrange(media_mask, 'b i -> b 1 i 1') == repeat(media_time, 'j -> 1 1 1 (j m)', m=64)\n",
    "sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n",
    "logits = sim.softmax(dim=-1)\n",
    "text_without_media_mask = media_mask == 0\n",
    "text_without_media_mask = rearrange(text_without_media_mask, 'b i -> b 1 i 1')\n",
    "logits = logits.masked_fill(text_without_media_mask, 0.)\n",
    "logits = einsum('... i j, ... j d -> ... i d', logits, v)\n",
    "logits = rearrange(logits, 'b h n d -> b n (h d)')\n",
    "y = out(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2048, 4096])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_embeddings torch.Size([1, 3, 1, 49, 2048])\n",
      "torch.Size([1, 3, 1, 49, 2048])\n",
      "vf.dim torch.Size([1, 3, 64, 2048])\n",
      "vs_shape torch.Size([1, 192, 2048])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (192x2048 and 64x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m magma(images,captions)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [5], line 34\u001b[0m, in \u001b[0;36mMagma.forward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     28\u001b[0m     x_attn_block \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m     29\u001b[0m         lm\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh[l], \u001b[39m'\u001b[39m\u001b[39mln_1\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m     x_attn_block\u001b[39m.\u001b[39mperceiver_pipe(\n\u001b[1;32m     32\u001b[0m         visual_features, media_mask\u001b[39m=\u001b[39mmedia_mask)\n\u001b[0;32m---> 34\u001b[0m lm_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm(\n\u001b[1;32m     35\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minput_embeddings,\n\u001b[1;32m     36\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:920\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, embs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[39m    ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[39m    ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 920\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    921\u001b[0m     input_ids,\n\u001b[1;32m    922\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    923\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    924\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    925\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    926\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    927\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    928\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    929\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    930\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    931\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    932\u001b[0m     embs\u001b[39m=\u001b[39;49membs,\n\u001b[1;32m    933\u001b[0m )\n\u001b[1;32m    934\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    936\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:755\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, embs)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39minputs, use_cache, output_attentions)\n\u001b[1;32m    753\u001b[0m         \u001b[39mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 755\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[1;32m    756\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    757\u001b[0m         hidden_states,\n\u001b[1;32m    758\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    759\u001b[0m         attn_mask,\n\u001b[1;32m    760\u001b[0m         head_mask[i],\n\u001b[1;32m    761\u001b[0m     )\n\u001b[1;32m    762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    764\u001b[0m         hidden_states,\n\u001b[1;32m    765\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    770\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:751\u001b[0m, in \u001b[0;36mGPTNeoModel.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_forward\u001b[39m(\u001b[39m*\u001b[39minputs):\n\u001b[1;32m    750\u001b[0m     \u001b[39m# None for past_key_value\u001b[39;00m\n\u001b[0;32m--> 751\u001b[0m     \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39;49minputs, use_cache, output_attentions)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:439\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    431\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    437\u001b[0m ):\n\u001b[1;32m    438\u001b[0m     residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 439\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(hidden_states)\n\u001b[1;32m    440\u001b[0m     attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\n\u001b[1;32m    441\u001b[0m         hidden_states,\n\u001b[1;32m    442\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    448\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 98\u001b[0m, in \u001b[0;36mGatedCrossAttentionBlock.forward\u001b[0;34m(self, embs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embs: TensorType[\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSequence Length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTokenDim\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> 98\u001b[0m     x_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_attn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_features, embs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmedia_mask)\n\u001b[1;32m     99\u001b[0m     attn_out \u001b[39m=\u001b[39m embs \u001b[39m+\u001b[39m tanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh1) \u001b[39m*\u001b[39m x_attn\n\u001b[1;32m    100\u001b[0m     x_ffw \u001b[39m=\u001b[39m attn_out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffw(x_attn) \u001b[39m*\u001b[39m tanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh2)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [1], line 68\u001b[0m, in \u001b[0;36mMaskedCrossAttention.forward\u001b[0;34m(self, latent, y, media_mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m visual_features \u001b[39m=\u001b[39m rearrange(latent, \u001b[39m'\u001b[39m\u001b[39mb t n d -> b (t n) d\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvs_shape\u001b[39m\u001b[39m\"\u001b[39m, visual_features\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 68\u001b[0m k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_k_w(visual_features)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_w(y)\n\u001b[1;32m     70\u001b[0m q, k, v \u001b[39m=\u001b[39m rearrange_many((q, k, v), \u001b[39m'\u001b[39m\u001b[39mb n (h d) -> b h n d\u001b[39m\u001b[39m'\u001b[39m, h\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (192x2048 and 64x1024)"
     ]
    }
   ],
   "source": [
    "magma(images,captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_images = encoder(flat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 49, 2048])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m flat_img \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, img_shape[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m], img_shape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], img_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m featured_images \u001b[39m=\u001b[39m encoder(flat_img)\n\u001b[0;32m----> 4\u001b[0m featured_images\u001b[39m.\u001b[39;49mview(img_shape[\u001b[39m0\u001b[39;49m] , img_shape[\u001b[39m1\u001b[39;49m], img_shape[\u001b[39m3\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "img_shape = images.shape\n",
    "flat_img = images.view(-1, img_shape[-3], img_shape[-2], img_shape[-1])\n",
    "featured_images = encoder(flat_img)\n",
    "featured_images.view(img_shape[0] , img_shape[1], img_shape[3], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, -1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(img_shape[0] , img_shape[1], img_shape[3], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dd78b44acf79f242ecca224d05742a1b089d3d2b8416bfe26aa6b4466f63f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
