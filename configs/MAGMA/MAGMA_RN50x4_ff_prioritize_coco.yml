{
    #finetune specific settings
    load_optimizer: false,    
    weight_decay: 0.1,
    save_every: 500,

    # image encoder settings
    encoder_name: 'clip_resnet',
    adapter_config: {"mlp": {"adapter_type": "normal", "downsample_factor": 6}, "attention": {"adapter_type": "normal", "downsample_factor": 12}},
    freeze_img_encoder: false,
    
    # train settings 
    batch_size: 256,
    train_steps: 5000,
    lr: 7.0e-4,  
    min_lr: 0.0,
    lr_decay_iters: 6000,
    image_enc_lr: 1.5e-6,
    use_image_embed_layernorm: true,
    image_embed_dropout_prob: 0.1, 
    freeze_img_encoder_batchnorms: false,
    image_size: 384,
    eval_steps: 256,

    gradient_accumulation_steps: 8,
    zero_stage: 2,
    gradient_clipping: 1.0,

    # dataset / save / load settings
    dataset_type: 'new',
    train_dataset_dir: '/mnt/localdisk/coco_converted',
    eval_dataset_dir: '/mnt/localdisk/coco_val_converted',

    vqa_dir: "/mnt/localdisk/vqa_val_converted",
    gqa_dir: "/mnt/localdisk/gqa_val_converted",
    
    save: "/mnt/shared_vol/checkpoints/MAGMA_RN50x4_ff_prioritize_coco",
    load: "/mnt/shared_vol/checkpoints/MAGMA_RN50x4_ff_prioritize",

    eval_every: 100,
    wandb_project: "MAGMA_training",
    name: "MAGMA_RN50x4_ff_prioritize_coco"
}
# bash ~/gpt-neox/tools/syncdir.sh /mnt/localdisk/vizwiz